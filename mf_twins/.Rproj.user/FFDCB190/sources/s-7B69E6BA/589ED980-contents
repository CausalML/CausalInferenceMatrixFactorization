library(onehot)
#data_dir = "/home/xm77/causal/JL_sim/Data/twins/"
data_dir = "/Users/xiaojicmao/Dropbox/research/causal inference/simulation/JL_sim/Data/twins/"

data_t = read.csv(paste0(data_dir, "twin_pairs_T_3years_samesex.csv"))[, -1]
data_y = read.csv(paste0(data_dir, "twin_pairs_Y_3years_samesex.csv"))[, -1]
data_x = read.csv(paste0(data_dir, "twin_pairs_X_3years_samesex.csv"))[, -c(1, 2)]

# args <- commandArgs(TRUE)
# dev_size <- as.numeric(args[1])

# replace bord_0 and bord_1
data_x[data_x["bord_0"] == 1 & !is.na(data_x["bord_0"]), "bord_0"] = 0
data_x[data_x["bord_0"] == 2 & !is.na(data_x["bord_0"]), "bord_0"] = 1
data_x[data_x["bord_1"] == 1 & !is.na(data_x["bord_1"]), "bord_1"] = 0
data_x[data_x["bord_1"] == 2 & !is.na(data_x["bord_1"]), "bord_1"] = 1
# 0,1,2,3,..,m-1--> 1,2,3,..,m
VAR = c("nprevistq", "brstate_reg", "dfageq", "mplbir_reg", "orfath", 
        "ormoth", "cigar6", "drink5")
for (v in VAR){
  data_x[v] = sapply(data_x[v], function(x) x + 1)
}
# move some values
data_x[data_x["dtotord_min"] == 17 & !is.na(data_x["dtotord_min"]), "dtotord_min"] = 16 
data_x[data_x["dlivord_min"] == 15 & !is.na(data_x["dlivord_min"]), "dlivord_min"] = 14 
data_x[data_x["brstate"] == 55 & !is.na(data_x["brstate"]), "brstate"] = 53 
data_x[data_x["brstate"] == 57 & !is.na(data_x["brstate"]), "brstate"] = 54
data_x[data_x["brstate"] == 59 & !is.na(data_x["brstate"]), "brstate"] = 55
data_x[data_x["mplbir"] == 59 & !is.na(data_x["mplbir"]), "mplbir"] = 58


# delete irrelevant features
for (f in c('birmon', 'data_year', "infant_id_0", "infant_id_1", "stoccfipb", "stoccfipb_reg")){
  data_x[f] = NULL
}
na_frac = apply(is.na(data_x), 2, sum)/dim(data_x)[1]
# only consider infant pairs whose weights are less than 2kg 
index = data_t[1] < 2000 & data_t[2] < 2000
data_t = data_t[index, ]
data_x = data_x[index, ]
data_y = data_y[index, ]
sum(is.na(data_x))/(dim(data_x)[1]*dim(data_x)[2]) # original dataset: 10% of missing values 
original_data = cbind(data_y, data_t, data_x)
write.csv(original_data, paste0(dir, "Data/twins/original_data.csv"))


#nonmiss = colnames(data_x)[na_frac == 0][-1]
#seed1 = sample(100000,1)

# confounders = data_x[nonmiss]
# for (name in names(confounders)){
#   confounders[, name] = as.factor(confounders[, name])
# }
# confounders = predict(onehot(confounders), confounders)
confounders = data_x["gestat10"]

# seed1=32004
# set.seed(seed1)
# conf_coef = rnorm(ncol(confounders))
# prob = 1/(1 + exp(-as.matrix(confounders)%*%conf_coef))
prob = sigmoid(5*(confounders/10 - 0.1))

seed2 = sample(100000,1)
set.seed(seed2)
treatment = ifelse(runif(dim(data_x)[1]) <= prob, 1, 0)
colnames(treatment) = "t"
table(treatment)
#data_t = ifelse(treatment == 1, data_t[, 2], data_t[, 1])
data_y = ifelse(treatment == 1, data_y[, 2], data_y[, 1])
colnames(data_y) = "y"
data_bord = ifelse(treatment == 1, data_x[, "bord_1"], data_x[, "bord_0"])
colnames(data_bord) = "bord"
data_x[c("bord_1", "bord_0")] = NULL

comp_data = cbind(data.frame(y = data_y, t = treatment, bord = data_bord), data_x)

# split development dataset 
# n = dev_size
# index = sample(dim(comp_data)[1], n, replace = F)
# dev_data = comp_data[index, ]
# write.csv(dev_data, paste0(data_dir, "dev_data.csv"))
write.csv(comp_data, paste0(data_dir, "comp_data.csv"))

# dev = read.csv(paste0(data_dir, "dev_data.csv"))
#comp = read.csv(paste0(data_dir, "comp_data.csv"))
# data = read.csv(paste0(data_dir, "comp_data.csv"))[, -1]
